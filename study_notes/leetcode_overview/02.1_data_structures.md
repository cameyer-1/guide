### **Part 2: Mastering the Technical Core (The "LeetCode" Part)**

This is the bread and butter of the coding interview. However, a senior-level performance isn't about memorizing solutions to hundreds of problems. It is about deeply understanding the tools—the data structures and algorithms—and demonstrating the ability to select the right one for the job based on a rigorous analysis of its trade-offs.

#### **2.1 Data Structures Aren't Just Tools, They're Decisions**

Your choice of data structure is the single most impactful decision you will make in a coding interview. It dictates the performance, complexity, and even the feasibility of your entire algorithm. You must be able to justify your choice from first principles, starting with the most fundamental structures.

---

### **2.1.1 Arrays & Strings: The Bedrock and Their Hidden Costs**

Arrays (or in Python, `lists`) are the default collection for many, but this familiarity hides significant performance traps. Understanding their underlying implementation is non-negotiable.

**Under the Hood: Contiguous Memory**

The defining characteristic of an array or a standard Python `list` is that its elements (or pointers to its elements) are stored in a single, unbroken block of memory. This physical layout is the direct cause of all its performance characteristics.

*   **Python Specifics: A `list` is a Dynamic Array of Pointers:**
    Unlike a C array that stores values directly, a Python `list` stores references (pointers) to the objects. This means `my_list = [1, "hello", MyObject()]` is a contiguous block of memory containing the *memory addresses* of the integer object `1`, the string object `"hello"`, and your custom object instance. This adds a layer of indirection but is key to Python's flexibility.

**Core Operations and Their Unavoidable Costs**

*   **Indexing (Read/Write): `value = my_list[i]` or `my_list[i] = new_value`**
    *   **Complexity: O(1)**
    *   **The Why:** This is the primary benefit of contiguous memory. To access index `i`, the system performs a simple calculation: `start_address + i * size_of_element_pointer`. This is a single, instantaneous operation, regardless of the list's size. You must be able to explain this.

*   **Append at the End: `my_list.append(value)`**
    *   **Complexity: O(1) Amortized**
    *   **The Why:** "Amortized" is a key concept to discuss. Python `lists` are dynamic, meaning they resize automatically. When you create a list, Python allocates more space than immediately needed. Appending an item is O(1) as long as there is spare capacity. When the capacity is full, the list must be resized. This is an O(N) operation: a new, larger block of memory is allocated, and every single element from the old block is copied to the new one.
    *   **Senior-level explanation:** "While a single `append` can be O(N) in the worst case during a resize, this cost is spread out over many O(1) appends. On average, each append operation contributes a constant amount of work, making it amortized O(1)."

*   **Insertion/Deletion in the Middle: `my_list.insert(i, value)` or `del my_list[i]`**
    *   **Complexity: O(N)**
    *   **The Why:** This is a major performance trap. To insert an element at index `i`, every element from `i` to the end of the list must be shifted one position to the right to make space. To delete an element at `i`, every element from `i+1` to the end must be shifted one position to the left. The cost is proportional to the number of elements shifted, which is O(N) in the worst and average case. If your algorithm requires frequent insertions or deletions from the front or middle, a `list` is almost always the wrong choice.

*   **Searching: `value in my_list`**
    *   **Complexity: O(N)**
    *   **The Why:** With no information about the data, the only way to check for an element's existence is to perform a linear scan—checking each element one by one from the beginning until it's found or the list ends.

*   **Slicing: `new_list = my_list[i:j]`**
    *   **Complexity: O(k)** where k is the size of the slice (j - i).
    *   **The Why:** Slicing creates a *new, shallow copy* of the list. It must allocate memory for the new list and then copy the `k` pointers from the original. This is a subtle but critical point; it is not a free operation, especially for large slices.

**Python Strings: Immutable Arrays of Characters**

Think of a string as a specialized, *immutable* array. Immutability is its most important property and the source of a classic performance pitfall.

*   **The Immutability Trap:** You cannot change a character in an existing string. Any operation that appears to "modify" a string actually creates an entirely new one.

*   **Example: String Concatenation in a Loop (A Performance Disaster)**
    ```python
    # AVOID THIS - O(N^2) complexity
    names = ["alex", "jacob", "meta", "netflix"]
    result = ""
    for name in names:
        result += name # Each step creates a new string
    ```
    In the loop above, if `result` has length `k` and `name` has length `m`, the operation `result + name` must allocate a new string of size `k+m` and copy both the old `result` and the new `name` into it. Doing this repeatedly in a loop of N items leads to O(N^2) performance, which is a severe inefficiency.

*   **The Correct Pattern: `str.join()`**
    ```python
    # PREFER THIS - O(N) complexity where N is total length of all strings
    names = ["alex", "jacob", "meta", "netflix"]
    result = "".join(names)
    ```    *   **The Why:** `"".join()` is highly optimized. It performs a single pre-computation pass over the list of strings to calculate the exact final size required. It then allocates one block of memory of that size and copies all the strings into it in a single pass. This is a fundamental Python idiom, and using it correctly signals experience.

**When to Avoid Arrays/Lists**

*   **Frequent Lookups by Value:** If your problem is dominated by `if x in my_collection:` or finding an element to get its index, a `list`'s O(N) search is suboptimal.
    *   **Better Choice:** Hash Map (`dict`) or `set` for O(1) average-case lookups.

*   **Frequent Insertions/Deletions at the Beginning or Middle:** The O(N) cost of shifting elements will kill performance.
    *   **Better Choice:** A doubly-linked list (`collections.deque` in Python) provides O(1) insertions and deletions at both ends.

*   **Producer-Consumer Queues:** If you need to add to one end and remove from the other (a FIFO queue), a list is inefficient for front-removals (`.pop(0)` is O(N)).
    *   **Better Choice:** `collections.deque` is purpose-built for this, with O(1) `append` and `popleft`. This is the canonical way to implement BFS in Python.

### **2.1.2 Linked Lists: When and Why (It's Not Just About Pointers)**

Linked lists are the canonical counterpoint to arrays. Where arrays provide fast indexing at the cost of slow insertions/deletions, linked lists offer the exact opposite. Misunderstanding their purpose and using them inappropriately is a red flag that a candidate only has surface-level knowledge of data structures.

**Under the Hood: The Node Chain**

The fundamental unit of a linked list is the `Node`, a simple object containing two pieces of information:
1.  The `value` it holds.
2.  A `next` pointer (or reference) to the subsequent `Node` in the sequence.

The list itself is simply a reference to the `head` node. Unlike an array, these nodes can be scattered anywhere in memory. This physical decoupling is the source of all their trade-offs. In an interview, you'll almost always be expected to define this structure yourself.

```python
# A classic Node definition for a Singly Linked List
class Node:
    def __init__(self, value=0, next_node=None):
        self.val = value
        self.next = next_node
```

**Core Operations and Their Unavoidable Costs**

*   **Indexing/Accessing an Element: `get(index)`**
    *   **Complexity: O(N)**
    *   **The Why:** This is the most significant drawback. There is no mathematical shortcut to find the `i`-th element. You *must* start at the `head` and sequentially traverse the `next` pointers `i` times. For large lists, this is prohibitively slow and is the primary reason linked lists are not a general-purpose replacement for arrays.

*   **Insertion/Deletion (at a known location):**
    *   **Complexity: O(1)**
    *   **The Why:** This is the primary advantage. If you have a pointer to the node *before* the desired modification point (let's call it `prev_node`), insertion or deletion is a simple reassignment of pointers. You don't need to shift subsequent elements.
        *   **Insertion:** Create `new_node`. Set `new_node.next = prev_node.next`. Set `prev_node.next = new_node`.
        *   **Deletion:** Set `prev_node.next = prev_node.next.next`.
    *   **The Caveat:** The cost lies in *getting to* the modification point. The O(1) complexity only applies if you already have the reference to the node you need to modify. Searching for the spot is still O(N).

*   **Insertion/Deletion at the Head:**
    *   **Complexity: O(1)**
    *   **The Why:** It's a trivial pointer change: the new node's `next` points to the old `head`, and the list's `head` reference now points to the new node. This efficiency is why linked lists are excellent for stacks (LIFO).

*   **Insertion/Deletion at the Tail:**
    *   **Singly Linked List:** O(N) to add, as you must traverse to the last node. It becomes O(1) if you explicitly maintain a `tail` pointer. Deletion is still O(N) as you must traverse to find the *second-to-last* node.
    *   **Doubly Linked List:** O(1) if you maintain a `tail` pointer, as the tail's `prev` pointer is immediately accessible.

**Singly vs. Doubly Linked Lists**

A doubly linked list node also contains a `prev` pointer to the preceding node.

*   **Trade-off: Memory.** Each node stores an extra pointer, which can be significant for lists with millions of small elements.
*   **Advantage: Flexibility.**
    1.  **Bidirectional Traversal:** You can navigate the list forward and backward.
    2.  **O(1) Deletion with only a pointer to the node itself.** In a singly linked list, if you want to delete a `node`, you need a pointer to `node.prev`. In a doubly linked list, `node.prev` is directly accessible, making the deletion a true O(1) operation. This property is crucial for algorithms like implementing an LRU Cache.

**The Pythonic Way: `collections.deque`**

While you might implement a linked list from scratch in an interview to prove you understand the mechanics, for practical coding, you should use `collections.deque`.

*   **What it is:** A highly optimized "deck" (double-ended queue) implemented as a doubly linked list of fixed-size blocks of arrays.
*   **Why use it:** It provides the primary benefits of a linked list—O(1) appends and pops from *both* the left and right ends—with better memory locality and performance than a pure node-by-node implementation.
*   **The takeaway:** When you need a queue (FIFO) or a stack (LIFO) in Python, do not use `list.pop(0)` (which is O(N)). Use `deque`. Using `deque.popleft()` correctly signals senior-level Python competency.

**When to Use a Linked List**

You choose a linked list when your application's access pattern fits its strengths and avoids its weaknesses.

1.  **As a Queue (FIFO) or Stack (LIFO):** Your primary operations are additions and removals from the ends. This is the most common use case. E.g., for implementing a Breadth-First Search in a graph. Here, a `deque` is the perfect tool.
2.  **When Insertions/Deletions are Frequent:** And when you can maintain direct references to the nodes you need to modify, avoiding the O(N) search cost. The classic example is an LRU cache, where elements are constantly moved from the middle to the front of the list.

**Critical Patterns and Techniques**

*   **The Two-Pointer Technique:** This is the most important pattern for linked list problems. Using two pointers moving at different speeds (`slow` and `fast`) allows you to solve a class of problems in a single pass.
    *   **Cycle Detection:** A `fast` pointer moves two steps at a time, a `slow` pointer moves one step. If they ever meet, there is a cycle.
    *   **Finding the Middle:** When the `fast` pointer reaches the end, the `slow` pointer will be at the middle.
*   **The Dummy Head Node:** When a problem involves modifications that might change the head of the list (e.g., removing the first element, inserting a new head), create a `dummy` or `sentinel` node that points to the real `head`. You perform all operations relative to this `dummy` node. This dramatically simplifies code by eliminating `if head is ...` edge case checks. Using this technique is a strong signal of clean, robust coding.

### **2.1.3 Hash Maps & Sets: Your Default, But Know the Dangers**

The Hash Map (or `dict` in Python) is arguably the most versatile and powerful tool in the standard data structure library. For any problem involving lookups, frequency counting, or establishing relationships between items, it should be your default consideration. However, treating it as a black box without understanding its failure modes and trade-offs is a significant gap in senior-level competency.

**Under the Hood: The Array + Hash Function Combination**

At its core, a hash map is a clever combination of an array and a hash function.
1.  **The Array:** A pre-allocated, contiguous block of memory, often called the "bucket array".
2.  **The Hash Function:** A deterministic function that takes a key as input and produces an integer hash code. This code is then mapped (e.g., using the modulo operator) to an index within the bucket array.

The goal is to turn any key—be it a string, a number, or a tuple—into an array index, allowing you to leverage the O(1) lookup power of an array. Python's built-in `dict` and `set` are highly optimized implementations of this concept.

**Core Operations and Their Unavoidable Costs**

*   **Insertion, Deletion, and Lookup (`my_dict[key] = v`, `del my_dict[key]`, `v = my_dict[key]`, `key in my_dict`)**
    *   **Complexity: O(1) Average, O(N) Worst Case**
    *   **The "Why" - Average Case (O(1)):** In the ideal scenario, the hash function distributes keys evenly across the array. An operation involves a three-step process, all of which are constant time:
        1.  Compute the key's hash value.
        2.  Map the hash to an index in the underlying array.
        3.  Access that array index.
    *   **The "Why" - Worst Case (O(N)) - HASH COLLISIONS:** This is the most important concept to articulate. It's impossible for a hash function to guarantee a unique index for every possible key. A **collision** occurs when two distinct keys hash to the same array index.
        *   **How it's handled (Separate Chaining):** The most common strategy is to have each bucket in the array not be a single value, but rather a pointer to another data structure—typically a linked list—that holds all the key-value pairs that collided at that index.
        *   **The performance impact:** When you look up a key, you first hash to the correct bucket (O(1)) and then must traverse the linked list in that bucket (O(k), where k is the number of keys in the list) to find the one that matches your key exactly.
        *   **The worst-case scenario:** If you use a poor hash function or are subject to adversarial input, *all N keys* could hash to the same bucket. This degenerates the hash map into a single linked list, and every operation (insert, delete, lookup) becomes a linear O(N) search. You must be able to explain this failure mode.

**Hashable Keys: The Critical Prerequisite**

You cannot use just any data type as a key in a Python `dict` or an element in a `set`. A key must be **hashable**. This means two things:
1.  It must have a `__hash__` method that returns an integer and is consistent over the object's lifetime.
2.  It must have an `__eq__` method to compare for equality. If two objects are equal (`a == b`), they must have the same hash (`hash(a) == hash(b)`).

In practice, this means **keys must be immutable**.

*   **Valid Keys:** `int`, `str`, `float`, `bool`, `tuple` (as long as the tuple's elements are themselves immutable).
*   **Invalid Keys:** `list`, `dict`, `set`. These are mutable. Their contents can change, which would change their hash value, breaking the internal consistency of the hash map. Attempting to use one will raise a `TypeError`.

This is a fundamental Python constraint, and being aware of it (e.g., knowing you can use a tuple of strings as a key but not a list of strings) is a sign of experience.

**Hash Maps (`dict`) vs. Hash Sets (`set`)**

These are two sides of the same coin, and your choice between them communicates your intent.

*   **Hash Map (`dict`):** A collection of **key-value pairs**. It answers the question, "What is the value associated with this key?"
    *   *Use Case:* Frequency counting. `counts = {'apple': 3, 'orange': 5}`.

*   **Hash Set (`set`):** A collection of **unique keys**. It answers the question, "Does this key exist in my collection?" The values are irrelevant.
    *   *Use Case:* Checking for duplicates. `seen = {'apple', 'orange'}`.

Their underlying implementation and performance are virtually identical. Choosing a `set` when you don't need values signals that you are selecting the most precise tool for the job.

**When to Use Hash Maps/Sets**

This should be your knee-jerk reaction for problems characterized by "uniqueness," "counting," or "grouping."

*   **Frequency Counting:** The canonical tool. Problem: "Find the most common element in a list." Solution: Iterate through the list, using a `dict` to store element-to-count mappings.
*   **Checking for Duplicates / Seen Elements:** A `set` provides O(1) average time `add` and `in` operations, making it vastly superior to an O(N) list scan. Problem: "Contains Duplicate". Solution: Iterate through the list, adding elements to a `set`. If you try to add an element that's already there, you've found a duplicate.
*   **Fast Lookups:** Any time you need to "look up" information tied to an item. Problem: "Two Sum". Solution: Store numbers you've seen and their indices in a `dict` to instantly check if `target - current_number` exists.

**When to Be Wary**

*   **Ordering:** While Python `dict`s since version 3.7 maintain insertion order, you should not rely on this as a core algorithmic property unless you explicitly state your Python version dependency. If an algorithm requires sorted key traversal, you must explicitly sort the keys (`for key in sorted(my_dict): ...`), which is an O(N log N) operation, or use a data structure designed for ordering, like a balanced binary search tree (which Python does not have natively).
*   **Memory Usage:** The convenience of a `dict` comes with a cost. The underlying array must have empty slots to keep the collision rate low (this is managed via a "load factor"), and both the keys and values must be stored. This leads to significantly higher memory usage than a simple `list`. For very large datasets with dense integer keys (e.g., from 0 to 1,000,000), a plain `list`/array may be more memory-efficient.
*   **Custom Objects as Keys:** While powerful, implementing `__hash__` and `__eq__` on your own objects correctly can be tricky and is a source of bugs if not handled with care.

### **2.1.4 Trees, Tries, & Heaps: Hierarchical Data and Prioritization**

When data isn't linear, arrays and linked lists become inefficient. Trees and their specialized variants (Tries, Heaps) are designed to handle hierarchical relationships, sorted order, and prioritization. Failing to distinguish between these or recognize the problems they solve is a serious knowledge gap.

---

#### **I. Trees (The General Case: Binary Search Trees)**

A tree is a non-linear structure of nodes connected by edges, representing a hierarchy. The most common and foundational type you'll encounter is the Binary Search Tree (BST).

**The Binary Search Tree (BST) Invariant**

The power of a BST comes from one strict, non-negotiable rule that must be maintained at all times:
*   For any given node `N`, all values in its left subtree are **less than** `N`'s value.
*   For any given node `N`, all values in its right subtree are **greater than** `N`'s value.

This invariant is what allows for efficient searching. In an interview, you'll be expected to define the node structure yourself.

```python
class TreeNode:
    def __init__(self, val=0, left=None, right=None):
        self.val = val
        self.left = left
        self.right = right
```

**Core Operations and The Critical Trade-off: Balanced vs. Unbalanced**

*   **Search, Insertion, Deletion:**
    *   **Complexity: O(H)**, where **H is the height of the tree**.
    *   **The "Why":** Every operation starts at the root and traverses down a single path. In the worst case, this path is the height of the tree. This `O(H)` complexity is meaningless without discussing the two states of a tree:

1.  **The Ideal Case: Balanced Tree**
    *   The tree maintains a bushy shape, ensuring the height `H` is logarithmic relative to the number of nodes `N`. Thus, **H ≈ log(N)**.
    *   All operations achieve **O(log N)** time.
    *   Self-balancing trees like AVL or Red-Black trees automatically perform rotations on insert/delete to enforce this balance. You are not expected to implement these in a typical interview, but you *are* expected to know they exist and that their purpose is to guarantee O(log N) performance.

2.  **The Worst Case: Degenerate Tree**
    *   If you insert elements in a sorted order (e.g., `1, 2, 3, 4, 5`) into a naive BST, you create a spindly chain of nodes, effectively a linked list.
    *   The height `H` becomes equal to the number of nodes `N`.
    *   All operations degrade to **O(N)**, completely negating the benefits of the tree structure.
    *   **This is a critical talking point.** Acknowledging this flaw shows you understand the theoretical limits and practical risks of using a naive BST.

**Tree Traversal Methods**

Knowing how to traverse a tree is fundamental. There are two main strategies: Depth-First Search (DFS) and Breadth-First Search (BFS).

*   **DFS (recursive or with a stack):**
    *   **In-Order (Left, Root, Right):** For a BST, this traversal visits the nodes *in sorted ascending order*. This is a crucial property.
    *   **Pre-Order (Root, Left, Right):** Useful for creating a copy of the tree or for "path"-like problems.
    *   **Post-Order (Left, Right, Root):** Useful when you need to process children before their parent (e.g., deleting nodes from a tree).
*   **BFS (with a queue, specifically `collections.deque`):**
    *   Also known as Level-Order Traversal. It visits all nodes at depth 0, then all nodes at depth 1, and so on. Ideal for finding the shortest path in an unweighted tree/graph.

**When to Use a BST:**

*   When you need an ordered data set that is frequently changing (inserts/deletes).
*   When you need to perform range queries (e.g., "find all numbers between 10 and 50").
*   As an alternative to a hash map when you also need sorted traversal.

---

#### **II. Tries (Prefix Trees)**

A Trie is a highly specialized tree optimized for storing and searching strings based on their prefixes.

**Under the Hood**

Each node in a Trie represents a single character. A path from the root to a node spells out a prefix. Nodes typically contain:
1.  A mapping from characters to child nodes (usually a `dict` in Python).
2.  A boolean flag (`is_end_of_word`) to indicate that the path to this node represents a complete word, not just a prefix.

```python
class TrieNode:
    def __init__(self):
        self.children = {}  # e.g., {'a': TrieNode(), 'b': TrieNode()}
        self.is_end_of_word = False
```

**Core Operations and Their Unbeatable Advantage**

*   **Insertion, Search (for a full word), `startsWith` (for a prefix):**
    *   **Complexity: O(L)**, where **L is the length of the string being processed**.
    *   **The "Why" and Key Insight:** The performance is independent of the number of words (`N`) stored in the trie. This is a massive improvement over O(N*L) for scanning a list of strings or even O(L) for hashing a string and searching in a hash set. You are just traversing L nodes down the tree.

**When to Use a Trie:**
The signal for using a Trie is unmistakable: the problem involves prefixes.
*   **Auto-complete / Type-ahead functionality.**
*   **Dictionary lookups / Spell checkers.**
*   Finding all words with a common prefix.

**Trade-offs:**
*   **Memory:** Tries can consume a lot of memory. Each node is a Python object containing a dictionary, which is far more overhead than storing strings in a simple `set`. This trade-off of memory for time is the core decision.

---

#### **III. Heaps (Priority Queues)**

A Heap is not for searching. Its sole purpose is to provide immediate, O(1) access to the minimum (or maximum) element in a collection.

**The Heap Property**

*   **Min-Heap (default in Python):** The value of any parent node is less than or equal to the value of its children. This recursively guarantees the root element is the absolute minimum of the entire heap.
*   **Max-Heap:** The value of a parent is greater than or equal to its children.

**Under the Hood: An Array**

While conceptually a binary tree, a heap is almost always implemented as a flat array (a Python `list`) for superior performance and memory efficiency. The parent-child relationships are calculated arithmetically, avoiding pointer overhead. For a parent at index `i`:
*   Left child is at `2*i + 1`
*   Right child is at `2*i + 2`

**The Pythonic Way: The `heapq` Module**

You do not implement a heap from scratch in an interview. You use Python's built-in `heapq` library, which proves you know the standard library.
*   `heapq` only provides a **min-heap**.
*   **To simulate a max-heap:** Insert the *negatives* of the values into the heap. The smallest negative number corresponds to the largest absolute value. This is a standard and expected trick.

**Core Operations**

*   **Find Minimum (`my_heap[0]`):**
    *   **Complexity: O(1)**. The minimum is always at the first index. This is the primary benefit.
*   **Insert (`heapq.heappush`):**
    *   **Complexity: O(log N)**. The new element is added to the end of the list, and then it "sifts up" or "bubbles up," swapping with its parent until the heap property is restored.
*   **Extract Minimum (`heapq.heappop`):**
    *   **Complexity: O(log N)**. The root element (at index 0) is returned. To fill the hole, the *last* element in the list is moved to the root. This breaks the heap property, so it then "sifts down," swapping with its smaller child until its correct position is found.

**When to Use a Heap:**
A heap is the answer when your algorithm depends on repeatedly processing the "next smallest" or "next largest" item.
*   **"Top K" problems:** "Find the Kth largest element in an array." Solution: Maintain a min-heap of size K. For each number, if it's larger than the smallest item in the heap (`heap[0]`), pop that item and push the new one. After iterating through all numbers, the heap contains the K largest elements, and its root is the Kth largest.
*   **Implementing graph algorithms:** Essential for Dijkstra's (shortest path) and Prim's (Minimum Spanning Tree).
*   **Scheduling:** "Process the task with the highest priority."
*   Merging sorted files/lists.

### **2.1.5 Graphs: The Real World Is a Graph, Model It Correctly**

Graphs are the most general and expressive data structure, capable of modeling nearly any scenario involving interconnected entities. Many interview problems that don't seem to be about graphs are, in fact, graph problems in disguise. Recognizing this is a hallmark of a senior engineer. Failure to model the problem correctly here means the rest of the solution is built on a flawed foundation.

**What is a Graph?**

A graph `G` is simply a collection of two things:
1.  **Vertices (V):** A set of nodes or entities.
2.  **Edges (E):** A set of connections or relationships between pairs of vertices.

Unlike trees, graphs have fewer restrictions: an edge can connect any node to any other node, and cycles (paths that start and end at the same vertex) are common.

**Categorizing a Graph: The First Clarifying Questions**

When a problem implies a graph, you must first clarify its properties. Your choice of algorithm depends entirely on the answers.

*   **Directed vs. Undirected:** Is the relationship one-way or two-way? An edge `(u, v)` in a directed graph means you can go from `u` to `v`, but not necessarily back. In an undirected graph, the connection is symmetric.
    *   *Example:* A social media "follow" is a directed edge. A "friendship" is an undirected edge.
*   **Weighted vs. Unweighted:** Does each connection have a cost or distance associated with it?
    *   *Example:* A map of flight paths between cities is a weighted graph, where the weight could be distance, flight time, or cost. A network of web page links is typically unweighted.
*   **Cyclic vs. Acyclic:** Are there paths that loop back to their starting point? A graph with no directed cycles is called a **Directed Acyclic Graph (DAG)**. DAGs are incredibly important for modeling dependencies, prerequisites, or any process where tasks have an order.

**Representing a Graph in Code: The Core Decision**

In an interview, the graph is rarely provided as a clean `Graph` object. You must build it from the input (e.g., a list of pairs, a matrix). The way you choose to represent it in memory is a critical design decision with significant trade-offs.

#### **1. Adjacency List (The Default Choice)**

This is the most common and generally most efficient representation for the sparse graphs typical in interviews.

*   **Structure:** A dictionary (hash map) where each key is a vertex, and its value is a list (or set) of its neighbors.
*   **Python Implementation:**
    ```python
    # For an unweighted, directed graph from a list of edges
    # e.g., edges = [[0, 1], [0, 2], [1, 2]]
    
    from collections import defaultdict

    adj = defaultdict(list)
    for u, v in edges:
        adj[u].append(v)
    
    # If the graph were undirected, you add the reverse edge too:
    # adj[v].append(u)
    ```
    *Using `defaultdict(list)` is a signal of senior Python proficiency. It elegantly handles the case of adding an edge for a vertex seen for the first time, avoiding clumsy `if key not in dict:` checks.*

*   **Trade-offs:**
    *   **Pros:**
        *   **Space Efficient for Sparse Graphs:** You only store the edges that actually exist. If a graph has millions of vertices but only a few edges per vertex, the memory usage is proportional to `V + E`, not `V^2`.
        *   **Efficiently Get Neighbors:** Iterating through all neighbors of a vertex is `O(degree(v))`, which is fast. This is the most common operation in graph algorithms.
    *   **Cons:**
        *   **Checking for a Specific Edge is Slower:** To check if an edge `(u, v)` exists, you must go to `adj[u]` and then scan its list for `v`, which takes `O(degree(u))` time.

#### **2. Adjacency Matrix**

*   **Structure:** A 2D array (list of lists) of size `V x V`, where `matrix[u][v]` is `1` (or the edge weight) if an edge from `u` to `v` exists, and `0` otherwise.
*   **Python Implementation:**
    ```python
    # Assuming V vertices, numbered 0 to V-1
    matrix = [[0] * V for _ in range(V)] 
    
    for u, v in edges:
        matrix[u][v] = 1 # or some weight
        # If undirected: matrix[v][u] = 1
    ```

*   **Trade-offs:**
    *   **Pros:**
        *   **Fast Edge Lookup:** Checking for an edge `(u, v)` is an O(1) array lookup: `matrix[u][v]`.
    *   **Cons:**
        *   **Massive Space Complexity:** Requires O(V²) space, regardless of how many edges there are. This is prohibitive for graphs with thousands or millions of vertices and makes it the wrong choice for sparse graphs.
        *   **Inefficiently Get Neighbors:** To find all neighbors of `u`, you must iterate through its entire row in the matrix, taking O(V) time even if it only has two neighbors.

**Why Adjacency List is Usually Better**
Most real-world and interview graphs are **sparse** (the number of edges `E` is much smaller than the maximum possible, `V^2`). In these cases, the `O(V+E)` space complexity and efficient neighbor iteration of an adjacency list make it the superior choice. You should only consider an adjacency matrix if the graph is **dense** (E is close to V²) or if the problem requires extremely frequent checks for specific edges.

**Recognizing a Graph Problem**

The key is to look beyond the literal data types and see the relationships. The problem is a graph problem if it involves:
*   **Explicit Networks:** Social networks, computer networks, road maps.
*   **Dependencies/Prerequisites:** Course schedules ("course A must be taken before B"), software build dependencies, character order in an alien dictionary. These often form DAGs, and the problem is a **topological sort**.
*   **State Transitions:** "What is the minimum number of steps to transform word A into word B by changing one letter at a time?" Each word is a vertex, and an edge exists between two words if they are one letter apart. The problem is a **shortest path on an unweighted graph (BFS)**.
*   **Mazes or Grids:** Finding a path through a 2D grid is a graph problem. Each cell `(row, col)` is a vertex, with edges to its valid adjacent cells (up, down, left, right). This is another common BFS/DFS application.
